% Based on free LaTex template https://www.overleaf.com/latex/templates/modelo-tcc-computacao-atitus/dgwsczcmpczz
\documentclass[14pt]{article}

\usepackage[a4paper, total={6in, 9in}]{geometry}

\usepackage[english, russian]{babel}
\usepackage{graphicx, url}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english=american]{csquotes}
\usepackage{float}
\usepackage{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{listings}
\usepackage{inconsolata}
\usepackage{tabularray}

\usepackage[style=abnt]{biblatex}
\addbibresource{sbc-template.bib}


\title{Colorizer}
\author{Бабанский Виталий, Бакин Денис}

\begin{document}

\maketitle

\section{Введение}

Задача восстановления цветных изображений из черно-белых снимков является распространенной задачи и применяется, например,
при обновлении исторических снимков, которые были сделаны до изобретения цветной фотографии.
Более сложной постановкой той же задачи считается раскрашивание снимков NIR (near-infrared spectroscopy) ---
это снимки, где вместо количества видимого света фотосенсором камеры подсчитывается количестве фотонов с длиной волны
от 780 нм до 2500 нм, то есть выше видимого диапазона. Такая съемка применяется при низкой освещенности и
при съемке архитектурных объектов.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{resources/pisa_tower_3_colorspaces.jpg}
    \caption{Три пространства цветов: RGB, черно-белое и NIR}
    \label{fig:id_figura}
\end{figure}


\section{Постановка задачи}
Целью проекта является создание и обучение нейронной сети для получения цветных изображений по данным черно-белым изображениям,
а также провести ряд экспериментов, воспроизвести результаты выбранных статей и измерить полученное качество по набору метрик.


\section{Литература}
Список рассмотренных не окончательный. Включены только те статьи, идеи которых скорее всего будут использованы в реализации.

\subsection{Раскрашивание с подсказками}
\cite{GuidedImageColorization} предлагает архитектуру полносверточной нейронной сети, которая принимает на вход
черно-белое изображение и набор локальных и глобальных подсказок от пользователя. Сеть раскрашивает указанные пиксели так, как
скажет пользователь, а остальное изображение так, чтобы оно было наиболее естественным. Сеть показывает приемлемое качество
как бейзлайн: основная полносверточная нейросеть используется в некоторых других более сложных архитектурах. Отличная качество достигается
в особо сложных случаях, когда на снимке есть мелкий орнамент или цвета, которые сложно восстановить из контекста (воздушные шары, например).

\subsection{Instance colorization}
\cite{InstaColor} использует основную полносверточную нейронную сеть, из \cite{GuidedImageColorization}. Идея авторов заключается в генерации
ограничивающих прямоугольников (bounding boxes) вокруг известных объектов на изображении с помощью предтренированного детектора \cite{MaskCNN}.
Затем с помощью выбранного backbone раскрашиваются как вырезанные объекты, так и все изображение в целом. Затем на этапе карт признаков
модуль слияния "мягко" объединяет вырезанные раскрашенные объекты и полное раскрашенное изображение. Это дает улучшенные результаты по сравнению с прошлыми статьями
и относится к полностью автоматическому раскрашиванию черно-белых снимков.

\subsection{Cooperative colorization}
\cite{CoColor} авторы решают целых 2 проблемы: улучшают качество раскрашивания и предлагаются объединение и трансфер знаний модели между двумя доменами входных данных:
черно-белых и NIR изображений. В статье предлагается генерировать альтернативный домен по данному (NIR по черно-белому изображению или наоборот). Затем каждое
из изображений раскрашивается, результат объединяется. Поскольку такое количество генеративных сетей может отклоняться от ответа, авторы статьи предлагают множество дополнительных
ограничений для модели в виде функций потерь, которые требуют, чтобы раскрашенные изображения из разных доменов были очень похожи по структуре (ведь цвет ее не меняется).

\subsection{NIR-to-RGB Spectral Translation with Mamba}
\cite{ColorMamba} является лучшей на данный момент архитектурой для раскрашивания NIR изображений (на датасете NIR изображений с подготовленной валидационной выборкой
\cite{VCIP_2020_NIR_dataset}). Основа подхода заключается в построении двух наборов связанных модулей: сети для раскрашивания в пространство RGB,
сети для раскрашивания в пространство HSV, а также набора более компактных неглубоких подмодулей, описанных авторами статьи.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.05\textwidth]{resources/performance.pdf}
    \caption{Результаты работы ColorMambda \cite{ColorMamba}. Также приведено сравнение с CoColor \cite{CoColor}}
    \label{fig:id_figura}
\end{figure}


\section{Данные}
Авторы статей \cite{GuidedImageColorization}, \cite{InstaColor}, \cite{CoColor} использовали датасеты COCO \cite{COCO} и ImageNet \cite{ImageNet}.
Авторы статьи \cite{ColorMamba} использовали датасет NIR изображений с подготовленной валидационной выборкой \cite{VCIP_2020_NIR_dataset}.
Мы планируем использовать выборки из датасетов ImageNet, который содержит сфокусированные фотографии различных объектов, и COCO, который
содержит более общие сцены: архитектуры, природы.

Возможно, будут проведены эксперименты с созданием генерации черно-белого изображения по NIR данным. В этом случае к данным будет добавлен датасет
"RGB-NIR Scene Dataset".


\section{Метрики качества}
Для оценки качества раскрашивания снимков будем использовать набор метрик. По ним же будем сравнивать качество работы моделей.

\begin{itemize}
    \item \textbf{MSE}. Один из наиболее очевидных методов оценки близости предсказания к "верному" ответу.
    К недостаткам этой метрики можно отнести неразличимость мелкой зашумленности и отсутствия контроля за резкими переходами цветов,
    которые требуются при корректном раскрашивании изображений.
    $$
        MSE(I_1, I_2) = \sum_{x=1}^{W} \sum_{y=1}^{H} \frac{(I_1(x,y) - I_2(x,y))^2}{W \cdot H}
    $$

    \item \textbf{PSNR (Пиковое отношение сигнал/шум)}: PSNR измеряет качество цветных изображений, сравнивая пиксельные различия между оригиналом и
    раскрашенным изображением. Более высокие значения PSNR указывают на лучшее качество изображения с меньшими искажениями.
    $$
        PSNR(I_1, I_2) = 10 \cdot \log_{10} \left( \frac{MAX^2}{MSE(I_1, I_2)} \right)
    $$
    где $MAX$ --- максимальное значение пикселя (например, 255 для 8-битных изображений).

    \item \textbf{SSIM (Индекс структурного сходства)}: SSIM оценивает структурное сходство между оригиналом и раскрашенным изображением,
    учитывая яркость, контрастность и текстуру. Этот индекс предоставляет более точную для восприятия меру качества изображения по сравнению
    с метриками на основе пикселей, такими как PSNR.
    $$
        SSIM(I_1, I_2) = \frac{(2 \mu_1 \mu_2 + C_1)(2 \sigma_{12} + C_2)}{(\mu_1^2 + \mu_2^2 + C_1)(\sigma_1^2 + \sigma_2^2 + C_2)}
    $$
    где $\mu_1$ и $\mu_2$ --- средние значения яркости оригинала и раскрашенного изображения, $\sigma_1^2$ и $\sigma_2^2$ --- дисперсии,
    $\sigma_{12}$ --- ковариация, а $C_1$ и $C_2$ --- константы для стабилизации деления. По приведенной формуле метрика считается локально,
    а затем усредняется по всему изображению.


    \item \textbf{AE (Абсолютная ошибка)}: AE количественно оценивает абсолютное различие между соответствующими пикселями оригинала и
    раскрашенного изображения. Меньшие значения AE указывают на лучшую точность раскраски.
    $$
        AE(I_1, I_2) = \sum_{x=1}^{W} \sum_{y=1}^{H} \frac{|I_1(x,y) - I_2(x,y)|}{W \cdot H}
    $$


    \item \textbf{LPIPS (Обученное перцептуальное сходство изображений)} \cite{PerceptualMetric}: LPIPS оценивает перцептуальное сходство с использованием моделей
    глубокого обучения, фокусируясь на том, как человеческое зрение воспринимает различия между оригиналом и раскрашенным изображением.
    Более низкие значения LPIPS означают, что раскрашенным изображение более точно соответствует восприятию человека.
    $$
        LPIPS(I_1, I_2) = \frac{1}{N} \sum_{i=1}^{N} \| f_i(I_1) - f_i(I_2) \|_2^2
    $$
    где $f_i$ --- выходные данные $i$-го слоя предобученной модели, а $N$ --- количество слоев.
\end{itemize}


\section{Текущая идея}
Идея на момент написания отчета и вероятно изменится в будущем.

В качестве бейзлайна хочется реализовать полносверточную нейронную сеть из \cite{GuidedImageColorization} и провести ряд экспериментов, возможно,
с реализацией интерактивных подсказок в пользовательском интерфейсе. Затем хотелось бы реализовать одну из других рассмотренных статей и проверить
воспроизводимость результатов по выбранным метрикам.


\section{Бейзлайн}
В качестве бейзлайна согласно \cite{GuidedImageColorization} была выбрана полносверточная нейронная сеть --- UNet, которая изначально была
предложена для сегментации медицинских изображений, \cite{UNetForMedSegmentation}. На вход такая сеть получает одноканальное изображение
(более подробно преобразования изображений будут описаны в следующем разделе), затем применяет набор сверток с residual connections, сокращая
в текущей модицикации размер изображений в 8 раз по каждому измерению, после чего upscale свертками восстанавливает его исходные размер.

Выход сети --- двуканальное изображение, которое в изначальной статье интерпретировалось как распределение по двум классам каждого из пикселей:
наибольшая вероятность у того класса маски сегментации, к которому скорее всего приналежит текущий пиксель. В нашем бейзлайне двуканальный выход сети интерпретировался
как два нормированных цветовых канала.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{resources/u_net_architecture.png}
    \caption{UNet архитекрута (пример для 32х32 в середине сети). Каждый голубой прямоугольник соответствует
    многоканальной карте признаков}
    \label{fig:id_figura}
\end{figure}

\section{Обучающий пайплайн}
Опишем, какие преобразование применяются к изображениям.
\begin{enumerate}
    \item Читаем RGB изображение
    \item Преобразуем RGB изображение в цветовое пространство CIELab и разделяем на одиночный канал $L$ -- яркость пикселей, вход сети и 
        и на двуканальную матрицу $ab$
    \item нормируем $L$ и $ab$ на максимальные значение по каналам: $100$ и $255$ соответственно
    \item подаем $L$ на вход сети, получаем $ab$
    \item денормируем каналы
    \item совмещаем в трехнкальное CIELab изображение и преобразуем обратно в RGB
    \item показываем пользователю или сохраняем
\end{enumerate}

Для удобства разработки и постановки экспериментов используется написание и сохранение логов с помощью библиоткеи Wandb,
в репозитории сохраняется модульная архитектура Python файлов, почти везде выбран объектно-ориентированный подход. Например,
в классе \texttt{Trainer} одноименного модуля реализована все логика, связанная с обучением, дообучением, сохранением, загрузкой, тестированием моделей
с выбранной функцией потерь, оптимизатора и загрузчиков данных для обучения, валидации и тестирования.

Ссылка на репозиторий кода: \url{https://github.com/dfbakin/colorizer/tree/checkpoint-1-baseline}

\pagebreak
\printbibliography

\end{document}
